{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center"
   },
   "source": [
    "<img src=\"../images/demos/FIUM.png\" width=\"350px\" class=\"pull-right\" style=\"display: inline-block\">\n",
    "\n",
    "# Visión Artificial\n",
    "\n",
    "### 4º de Grado en Ingeniería Informática\n",
    "\n",
    "Curso 2020-2021<br>\n",
    "Prof. [*Alberto Ruiz*](http://dis.um.es/profesores/alberto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![concept map](../images/demos/concept_map.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [libro de Szeliski](http://szeliski.org/Book/drafts/SzeliskiBook_20100903_draft.pdf)\n",
    "\n",
    "\n",
    "- [OpenCV](https://opencv.org/), [tutoriales en Python](https://docs.opencv.org/4.1.0/d6/d00/tutorial_py_root.html), [documentación](https://docs.opencv.org/4.1.0/)\n",
    "\n",
    "- [libro](https://books.google.es/books?id=seAgiOfu2EIC&printsec=frontcover)\n",
    "\n",
    "- [libro1](https://books.google.es/books?id=9uVOCwAAQBAJ&printsec=frontcover), [libro2](https://books.google.es/books?id=iNlOCwAAQBAJ&printsec=frontcover)\n",
    "\n",
    "\n",
    "- [scikit-image](http://scikit-image.org/), [scikit-learn](http://scikit-learn.org)\n",
    "\n",
    "\n",
    "- [datasets](https://en.wikipedia.org/wiki/List_of_datasets_for_machine_learning_research#Image_data)\n",
    "\n",
    "\n",
    "- [Python](https://docs.python.org/3.6/)\n",
    "\n",
    "- [numpy](http://www.numpy.org/), [scipy](http://docs.scipy.org/doc/scipy/reference/)\n",
    "\n",
    "- [matplotlib](http://matplotlib.org/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prácticas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Preguntas frecuentes](FAQ.ipynb)\n",
    "\n",
    "\n",
    "- [Guión de las sesiones](guionpracticas.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Presentación (15/2/21)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[introducción](intro.ipynb), [instalación](install.ipynb), [Python](python.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Introducción a la asignatura\n",
    "\n",
    "- Repaso de Python, numpy y matplotib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Introducción a la imagen digital (22/2/21)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[imagen](imagen.ipynb), [gráficas](graphs.ipynb), [indexado/stacks](stacks.ipynb), [dispositivos de captura](captura.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Modelo pinhole. Campo de visión (FOV, *field of view*, parámetro $f$)\n",
    "\n",
    "- Imagen digital: rows, cols, depth, step. Planar or pixel order. Tipo de pixel: byte vs float\n",
    "\n",
    "- Color encoding: RGB vs YUV vs HSV\n",
    "\n",
    "- Coordendas de pixel, coordenadas normalizadas (indep de resolución), coordenadas calibradas (independiente del FOV).\n",
    "\n",
    "- Aspect ratio. Resize.\n",
    "\n",
    "- Manipulación: slice regions, \"stack\" de imágenes\n",
    "\n",
    "- primitivas gráficas\n",
    "\n",
    "- captura: webcams, cameras ip, archivos de vídeo, v4l2-ctl, etc. Load / save.\n",
    "\n",
    "- entornos de conda, pyqtgraph, pycharm, spyder\n",
    "\n",
    "- Herramientas: formatos de imagen, imagemagick, gimp, mplayer/mencoder/ffmpeg, mpv, gstreamer, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Segmentación por color (1/3/21)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[canales de color](color.ipynb), [histograma](histogram.ipynb), [efecto chroma](chroma.ipynb), [segmentación por color](colorseg.ipynb)\n",
    "<br>\n",
    "[cuantización de color](codebook.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Teoría del color\n",
    "\n",
    "- ROIs, masks, probability map, label map\n",
    "\n",
    "- Componentes conexas vs contornos.\n",
    "\n",
    "- inRange\n",
    "\n",
    "- Chroma key\n",
    "\n",
    "- Histograma, transformaciones de valor (brillo, contraste), ecualización\n",
    "\n",
    "- Histograma nD\n",
    "\n",
    "- Distancia entre histogramas. Reproyección de histograma\n",
    "\n",
    "- background subtraction\n",
    "\n",
    "- activity detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Filtros digitales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[filtros de imagen](filtros.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- lineal\n",
    "\n",
    "    - convolution\n",
    "    - máscaras para paso alto, bajo, etc.\n",
    "    - separabilidad\n",
    "    - integral image, box filter\n",
    "    - dominio frecuencial\n",
    "    - [inverse filtering](http://yuzhikov.com/articles/BlurredImagesRestoration1.htm), [Wiener](https://www.cis.rit.edu/class/simg782/lectures/lecture_16/lec782_05_16.pdf)\n",
    "\n",
    "\n",
    "- no lineal\n",
    "\n",
    "    - mediana\n",
    "    - min, max\n",
    "    - algoritmos generales\n",
    "\n",
    "\n",
    "- Gaussian filter\n",
    "\n",
    "    - separabilidad\n",
    "    - cascading\n",
    "    - Fourier\n",
    "    - scale space\n",
    "\n",
    "\n",
    "\n",
    "- [morphological operations](http://docs.opencv.org/master/d9/d61/tutorial_py_morphological_ops.html#gsc.tab=0)\n",
    "\n",
    "    - structuring element\n",
    "    - dilate, erode\n",
    "    - open, close\n",
    "    - gradient\n",
    "    - fill holes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  4. Detección de bordes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[detección de bordes](bordes.ipynb), [Canny nms en C](cannyC.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- gradiente: visualización como *vector field*\n",
    "\n",
    "- operador de Canny\n",
    "\n",
    "- transformada de Hough\n",
    "\n",
    "- Histograma de orientaciones del gradiente (HOG)\n",
    "\n",
    "- implementación simple de HOG\n",
    "\n",
    "- detección de *pedestrians*\n",
    "\n",
    "- face landmarks (dlib)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Flujo óptico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[elipse de incertidumbre](covarianza.ipynb), [optical flow](harris.ipynb)\n",
    "\n",
    "- elipse de incertidumbre\n",
    "\n",
    "- cross-correlation\n",
    "\n",
    "- corners (Harris)\n",
    "\n",
    "- Lucas-Kanade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. *Keypoints*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[keypoints](keypoints.ipynb), [bag of visual words](bag-of-words.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- modelo cuadrático\n",
    "\n",
    "- blobs / saddle points (Hessian)\n",
    "\n",
    "- SIFT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Análisis frecuencial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[análisis frecuencial](fourier.ipynb), [filtrado inverso](inversefilt.ipynb)\n",
    "\n",
    "(Conclusión del capítulo 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Reconocimiento de formas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[shapes](shapes.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- umbralización\n",
    "\n",
    "- análisis de regiones (componentes conexas, transformada de distancia)\n",
    "\n",
    "- manipulación de contornos\n",
    "\n",
    "- invariantes frecuenciales de forma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Otras técnicas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[textura](textura.ipynb), [transformada de distancia](transf-dist.ipynb), [varios](varios.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Clasificación de texturas mediante *LBP* ([Wang and He, 1990](http://www.academia.edu/download/46467306/0031-3203_2890_2990135-820160614-8960-12m30mo.pdf), [wiki](https://en.wikipedia.org/wiki/Local_binary_patterns))\n",
    "\n",
    "- Transformada de distancia\n",
    "\n",
    "- Detección de caras mediante *adaboost* ([Viola & Jones, 2001](https://en.wikipedia.org/wiki/Viola%E2%80%93Jones_object_detection_framework))\n",
    "\n",
    "- Herramientas para OCR (*[tesseract](https://github.com/tesseract-ocr)*)\n",
    "\n",
    "- Herramientas para códigos de barras y QR (*[zbar](http://zbar.sourceforge.net/)*)\n",
    "\n",
    "- Segmentación de objetos mediante *GrabCut* ([Rother et al. 2004](https://cvg.ethz.ch/teaching/cvl/2012/grabcut-siggraph04.pdf), [tutorial](http://docs.opencv.org/3.2.0/d8/d83/tutorial_py_grabcut.html))\n",
    "\n",
    "- Detección de elipses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. *Machine learning*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[machine learning](machine-learning.ipynb)\n",
    "\n",
    "- Repaso de *Machine Learning* y *Pattern Recognition*\n",
    "\n",
    "- Repaso de computación neuronal\n",
    "\n",
    "- Introducción a la redes convolucionales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. *Deep learning* en visión artificial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[modelos avanzados](deep.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Modelos preentrenados\n",
    "\n",
    "- YOLO\n",
    "\n",
    "- face recognition\n",
    "\n",
    "- openpose (body landmarks)\n",
    "\n",
    "- Transfer learning\n",
    "\n",
    "- Data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Coordenadas homogéneas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comenzamos el estudio de la geometría visual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[perspectiva](geovis.ipynb), [coordenadas homogéneas](coordhomog.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformaciones lineales\n",
    "\n",
    "- espacios lineales, vectores\n",
    "\n",
    "- transformaciones lineales, matrices\n",
    "\n",
    "- producto escalar (**dot** product)\n",
    "\n",
    "- producto vectorial (**cross** product)\n",
    "\n",
    "- puntos, rectas, planos, meet & join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Geometría del plano\n",
    "\n",
    "- coordenadas homogéneas\n",
    "\n",
    "- interpretación como rayos\n",
    "\n",
    "- puntos y rectas del plano\n",
    "\n",
    "- incidencia e intersección, dualidad\n",
    "\n",
    "- puntos del infinito, recta del infinito\n",
    "\n",
    "- manejo natural de puntos del infinito\n",
    "\n",
    "- horizonte de un plano"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. Transformaciones del plano"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[transformaciones del plano](transf2D.ipynb), [sistemas de ecuaciones](sistecs.ipynb), [transformaciones de dominio](lookup.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Desplazamientos, rotaciones, escalado uniforme, escalado general, proyectividad.\n",
    "\n",
    "- Grupos euclídeo, similar, afín, proyectivo.\n",
    "\n",
    "- Propiedades invariantes de cada grupo.\n",
    "\n",
    "- Representación como matriz homogénea $3\\times 3$ y tipos de matriz de cada grupo.\n",
    "\n",
    "- *Cross ratio* de 4 puntos en una recta. De 5 rectas.\n",
    "\n",
    "- Estimación de transformaciones a partir de correspondencias.\n",
    "\n",
    "- Aplicaciones: rectificación de planos, mosaico de imágenes.\n",
    "\n",
    "- Transformaciones de dominio (deformaciones), lookup table.\n",
    "\n",
    "Avanzado\n",
    "\n",
    "- Transformación de rectas. Covarianza y contravarianza.\n",
    "\n",
    "- Cónicas: incidencia, tangencia, (pole-polar), cónica dual, transformación.\n",
    "\n",
    "- Objetos invariantes en cada grupo de transformaciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14. Modelo de cámara"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[modelo de la cámara](camera.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Espacio proyectivo: puntos y líneas 3D, planos, grados de libertad, plano del infinito, analogía con 2D.\n",
    "\n",
    "- Grupos de transformaciones 3D: y sus invariantes.\n",
    "\n",
    "- Modelo pinhole (proyección), cámara oscura, lente.\n",
    "\n",
    "- Transformación de perspectiva: proyección $\\mathcal P^3 \\rightarrow\\mathcal P ^2$.\n",
    "\n",
    "- cámara calibrada C=PRT, 6 dof, parámetros extrínsecos o pose.\n",
    "\n",
    "- calibración, distorsión radial.\n",
    "\n",
    "- Matriz de cámara estándar $M=K[R|t]$.\n",
    "\n",
    "- Matriz de calibración $K$ y campo visual.\n",
    "\n",
    "- PnP (*pose from n points*).\n",
    "\n",
    "- Realidad aumentada.\n",
    "\n",
    "- Anatomía de la cámara\n",
    "\n",
    "- Rotaciones sintéticas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15. Visión estéreo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[stereo](stereo.ipynb), [stereo-challenge](stereo-challenge.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Triangulación\n",
    "\n",
    "- Geometría epipolar\n",
    "\n",
    "- Extracción de cámaras\n",
    "\n",
    "- Rectificación estéreo\n",
    "\n",
    "- Mapas de profundidad\n",
    "\n",
    "\n",
    "Experimentos\n",
    "\n",
    "- Reproduce los experimentos con un par estéreo tomado con tu propia cámara usando el *tracker* de puntos estudiado en una clase anterior.\n",
    "\n",
    "- Intenta poner en marcha el sistema [VisualSFM](http://ccwu.me/vsfm/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [introducción](intro.ipynb)\n",
    "1. [instalación](install.ipynb)\n",
    "1. [Python](python.ipynb)\n",
    "\n",
    "1. [dispositivos de captura](captura.ipynb)\n",
    "\n",
    "1. [imagen](imagen.ipynb)\n",
    "1. [gráficas](graphs.ipynb)\n",
    "1. [canales de color](color.ipynb)\n",
    "\n",
    "1. [indexado, stacks](stacks.ipynb)\n",
    "1. [histograma](histogram.ipynb)\n",
    "1. [efecto chroma](chroma.ipynb)\n",
    "1. [segmentación por color](colorseg.ipynb)\n",
    "1. [cuantización de color](codebook.ipynb)\n",
    "\n",
    "1. [transformaciones de dominio](lookup.ipynb) \n",
    "\n",
    "1. [filtros de imagen](filtros.ipynb)\n",
    "1. [análisis frecuencial](fourier.ipynb)\n",
    "1. [filtrado inverso](inversefilt.ipynb)\n",
    "\n",
    "1. [transformada de distancia](transf-dist.ipynb)\n",
    "\n",
    "1. [detección de bordes](bordes.ipynb)\n",
    "\n",
    "1. [técnicas auxiliares](ipmisc.ipynb)\n",
    "1. [Canny nms en C](cannyC.ipynb)\n",
    "\n",
    "1. [elipse de incertidumbre](covarianza.ipynb)\n",
    "1. [optical flow](harris.ipynb)\n",
    "\n",
    "1. [keypoints](keypoints.ipynb)\n",
    "1. [bag of visual words](bag-of-words.ipynb)\n",
    "\n",
    "1. [machine learning](machine-learning.ipynb)\n",
    "1. [deep learning](deep.ipynb)\n",
    "1. [tensorflow](tensorflow.ipynb)\n",
    "\n",
    "1. [sistemas de ecuaciones](sistecs.ipynb)\n",
    "\n",
    "1. [textura](textura.ipynb)\n",
    "\n",
    "1. [shapes](shapes.ipynb)\n",
    "\n",
    "1. [varios](varios.ipynb)\n",
    "\n",
    "1. [perspectiva](geovis.ipynb)\n",
    "1. [coordenadas homogéneas](coordhomog.ipynb)\n",
    "1. [transformaciones del plano](transf2D.ipynb)\n",
    "1. [DLT](DLT.ipynb)\n",
    "\n",
    "1. [modelo de cámara](camera.ipynb)\n",
    "\n",
    "1. [visión stereo](stereo.ipynb)\n",
    "1. [stereo-challenge](stereo-challenge.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplos de código"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [`hello.py`](../code/hello.py): lee imagen de archivo, la reescala, muestra y sobreescribe un texto.\n",
    "\n",
    "1. [`webcam.py`](../code/webcam.py): muestra la secuencia de imágenes capturadas por una webcam.\n",
    "\n",
    "1. [`2cams.py`](../code/2cams.py): combina las imágenes tomadas por dos cámaras.\n",
    "\n",
    "1. [`surface.py`](../code/surface.py): superficie 3D de niveles de gris en vivo usando pyqtgraph.\n",
    "\n",
    "1. [`stream.py`](../code/stream.py): ejemplo de uso de la fuente genérica de imágenes \"mkStream\".\n",
    "\n",
    "1. [`video_save.py`](../code/video_save.py): ejemplo de uso de la utilidad de grabación de vídeo.\n",
    "\n",
    "1. [`mouse.py`](../code/mouse.py): ejemplo de captura de eventos de ratón.\n",
    "\n",
    "1. [`trackbar.py`](../code/trackbar.py): ejemplo de parámetro interactivo.\n",
    "\n",
    "1. [`deque.py`](../code/deque.py): procesamiento de las $n$ imágenes más recientes.\n",
    "\n",
    "1. [`histogram.py`](../code/histogram.py): histograma en vivo con opencv.\n",
    "\n",
    "1. [`histogram2.py`](../code/histogram2.py): histograma en vivo con matplotlib.\n",
    "\n",
    "1. [`inrange.py`](../code/inrange.py): umbralización de color, máscaras, componentes conexas y contornos.\n",
    "\n",
    "1. [`backsub.py`](../code/backsub.py): eliminación de fondo mediante MOG2.\n",
    "\n",
    "1. [`server.py`](../code/server.py): ejemplo de servidor web de imágenes capturadas con la webcam.\n",
    "\n",
    "1. [`mjpegserver.py`](../code/mjpegserver.py): servidor de secuencias de video en formato mjpeg.\n",
    "\n",
    "1. [`bot`](../code/bot): bots de [Telegram](https://python-telegram-bot.org/).\n",
    "\n",
    "1. [`reprohist.py`](../code/reprohist.py),  [`mean-shift.py`](../code/mean-shift.py), [`camshift.py`](../code/camshift.py): reproyección de histograma y tracking.\n",
    "\n",
    "1. [`grabcut.py`](../code/grabcut.py): segmentación de objetos interactiva mediante GrabCut.\n",
    "\n",
    "1. [`spectral.py`](../code/spectral.py): FFT en vivo.\n",
    "\n",
    "1. [`thread`](../code/thread): captura y procesamiento concurrente.\n",
    "\n",
    "1. [`testC.py`](../code/testC.py), [`inC`](../code/inC): Interfaz C-numpy.\n",
    "\n",
    "1. [`hog/pedestrian.py`](../code/hog/pedestrian.py): detector de peatones de opencv.\n",
    "\n",
    "1. [`hog/facelandmarks.py`](../code/hog/facelandmarks.py): detector de caras y landmarks de dlib.\n",
    "\n",
    "1. [`hog/hog0.py`](../code/hog/hog0.py): experimentos con hog.\n",
    "\n",
    "1. [`regressor.py`](../code/regressor.py): predictor directo de la posición de una región.\n",
    "\n",
    "1. [`LK/*.py`](../code/LK): seguimiento de puntos con el método de Lucas-Kanade.\n",
    "\n",
    "1. [`SIFT/*.py`](../code/sift.py): demostración de la detección de keypoints y búsqueda de coincidencias en imágenes en vivo.\n",
    "\n",
    "1. [`shape/*.py`](../code/shape): reconocimiento de formas mediante descriptores frecuenciales.\n",
    "\n",
    "1. [`ocr.py`](../code/ocr.py): reconocimiento de caracteres impresos con tesseract/tesserocr sobre imagen en vivo.\n",
    "\n",
    "1. [`zbardemo.py`](../code/zbardemo.py): detección de códigos de barras y QR sobre imagen en vivo.\n",
    "\n",
    "1. [`code/DL`](../code/DL): Modelos avanzados de deep learning para visión artificial (inception, YOLO, FaceDeep, openpose).\n",
    "\n",
    "1. [`code/polygons`](../code/polygons) y [`code/elipses`](../code/elipses): Rectificación de planos en base a marcadores artificiales.\n",
    "\n",
    "1. [`stitcher.py`](../code/stitcher.py): construcción automática de panoramas.\n",
    "\n",
    "1. [`code/pose`](../code/pose): estimación de la matriz de cámara y realidad aumentada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los ejercicios se deben presentar en un notebook *jupyter* o en documento *pdf* que incluya el **código** realizado, una **explicación** detallada y **resultados** de funcionamieno con imágenes de evaluación **originales**. Se recomienda incluir información sobre tiempos de cómputo y limitaciones de las soluciones propuestas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El curso anterior se propusierion los siguientes ejercicios. Más adelante indicaremos los obligatorios y opcionales de este curso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Z**. Pon en marcha el entorno de trabajo, incluyendo entre otros, los módulos y herramientas jupyter, spyder/pycharm, opencv, numpy, matplolib."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FOV**. Estima de forma aproximada el campo de visión ([FOV](https://en.wikipedia.org/wiki/Angle_of_view)) y parámetro $f$ de tu cámara. Determina a qué altura habría que ponerla para obtener una vista cenital completa de un campo de baloncesto. Calcula la altura de una pelota sobre el suelo a partir de su tamaño en la imagen.\n",
    "\n",
    "Realiza una calibración precisa de tu cámara mediante múltiples imágenes de un *chessboard* y compara el parámetro $f$ obtenido con el resultado anterior.\n",
    "\n",
    "Si la cámara incluye metadatos en las imágenes y aparece la *Focal Length*, puedes comprobar si este valor es coherente con el que tú obtienes.\n",
    "\n",
    "[Más informacion](imagen.ipynb#Calibración)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MSAT**. Modifica los canales de color (tono, saturación, etc.) de una secuencia de imágenes en vivo con un \"trackbar\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DMOV**. Construye un detector de movimiento basado en una sencilla diferencia de imágenes sucesivas. Opcionalmente puedes hacer un generador que filtre la secuencia de imágenes dejando pasar solo los frames estáticos (o, alternativamente, los que han sufrido un cambio apreciable. En este caso no hace falta detectar las zonas con movimiento). Puedes apoyarte en `deque.py`. Opcional: a) Limitar la detección a una región de interés. b) Guardar 2 ó 3 segundos de la secuencia detectada en un archivo de vídeo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CHRO**. Implementa el efecto chroma con imágenes en vivo de la webcam. Pulsando una tecla se captura el fondo y los objetos que aparezcan se superponen en otra imagen o secuencia de video. Se trata de conseguir un resultado [como el que se muestra aquí](../images/demos/chroma.png), pero en vivo. Opcional: compara el resultado con el método automático de eliminación de fondo ilustrado en `backsub.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SROI**. Escribe una herramienta para seleccionar con el ratón varias regiones de interés (ROI), almacenando los recortes en una lista, y opcionalmente en disco. (Puedes usar la clase ROI que vimos en `mean-shift`.) Será útil en futuros ejercicios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HISTCOL**. Construye un clasificador de objetos en base a la similitud de los histogramas de color del ROI (de los 3 canales por separado). Apóyate en SROI. [Más información](FAQ.ipynb#Ejercicio-HISTCOL)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SEGCOL**. Implementa la segmentación por color usando reproyección de histogramas en un programa que admite como argumento a) una carpeta con trozos de imágen que sirven como muestras de color y b) otra imagen o secuencia de video que deseamos clasificar. El resultado puede ser un conjunto de máscaras para cada clase, o una \"imagen de etiquetas\", donde diferentes colores indican cada una de las regiones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ROBCOL**. Evalúa la robustez del programa `inrange.py` realizado en prácticas en una escena real tomada desde distintas posiciones y condiciones de iluminación. Amplíalo para tener en cuenta que el color rojo aparece en dos zonas del canal H y para seleccionar un intervalo de áreas permitidas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FIL**. Muestra el efecto de diferentes filtros sobre la imagen en vivo de la webcam. Selecciona con el teclado el filtro deseado y modifica sus posibles parámetros (p.ej. el nivel de suavizado) con las teclas de flecha. Es conveniente permitir la selección de un ROI para comparar el resultado del filtro con el resto de la imagen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DLIB**. Escribe una aplicación de tu invención que utilice los [marcadores de cara](https://ibug.doc.ic.ac.uk/resources/facial-point-annotations/) obtenidos por el *shape detector* disponible en [dlib](http://dlib.net/) (ejemplo `/hog/facelandmarks.py`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**VROT**. Estima de forma aproximada la velocidad angular de rotación de la cámara (grados/segundo) analizando las trayectorias obtenidas por el *tracker* de Lucas-Kanade (ejemplo `LK/lk_track.py`). [Más información](FAQ.ipynb#Ejercicio-VROT)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WEB**. Construye un servidor web sencillo usando [flask](http://flask.pocoo.org/) que muestre una cierta transformación, especificada en la url, de las imágenes tomadas con la cámara. Apóyate en `server.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SIFT**. Escribe una aplicación de reconocimiento de objetos (p. ej. carátulas de CD, portadas de libros, cuadros de pintores, etc.) con la webcam basada en el número de coincidencias de *keypoints*. [Más información](FAQ.ipynb#Ejercicio-SIFT)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SILU**. Escribe una aplicación de reconocimiento de siluetas con la webcam basado en descriptores frecuenciales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DELOGO**. Detecta el logotipo de una cadena de TV en una emisión por streaming y suprime los cortes publicitarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OCR**. Escribe una aplicación de reconocimiento de dígitos manuscritos con la webcam. Compara el clasificador gaussiano y la red convolucional explicada en el capítulo de machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANON**. Modifica el ejemplo de reconocimiento de caras `DL/facerec` para seleccionar caras pinchando con el ratón (o tomándolas de un directorio) para que cuando se reconozcan en las imágenes se oculten (emborronándolas o pixelizándolas)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CR**. Reproduce la demostración del cross-ratio de 4 puntos en una recta del tema de [transformaciones del plano](transf2D.ipynb#Invariantes). Marca tres puntos con el ratón y añade automáticamente tres más y el punto de fuga, suponiendo que en el mundo real los puntos están alineados y a la misma distancia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RECTIF**. Rectifica la imagen de un plano para medir distancias (tomando manualmente referencias conocidas). Por ejemplo, mide la distancia entre las monedas en `coins.png` o la distancia a la que se realiza el disparo en `gol-eder.png`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SWAP**. Intercambia dos cuadriláteros en una escena marcando manualmente los puntos de referencia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NFACE**. Implementa la normalización de caras explicada en la sección de [transformaciones afines](transf2D.ipynb#Transformaciones-afines)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DNI**. Sustituye la foto de un carnet que se observa en una imagen en vivo. Intenta conseguir un resultado [parecido a esto](../images/demos/dni.mp4)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SUDO**. Haz un programa capaz de rellenar un sudoku que se observa en una imagen en vivo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PANO**. Crea automáticamente un mosaico a partir de las imágenes en una carpeta. Las imágenes no tienen por qué estar ordenadas ni formar una cadena lineal y no sabemos el espacio que ocupa el resultado. El usuario debe intervenir lo menos posible. Recuerda que debe tratarse de una escena plana o de una escena cualquiera vista desde el mismo centro de proyección. Debes usar homografías. Compara el resultado con el que obtiene la utilidad de stitching de OpenCV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RA**. Crea un efecto de realidad aumentada dinámico. Los objetos virtuales deben cambiar de forma, posición o tamaño siguiendo alguna lógica, y el usuario puede observar la escena cambiante desde cualquier punto de vista moviendo la cámara alrededor del marcador."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STC**. Resuelve los problemas planteados en el notebook [stereo-challenge](stereo-challenge.ipynb)."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "nbTranslate": {
   "displayLangs": [
    "en",
    "es"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "es",
   "targetLang": "en",
   "useGoogleTranslate": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
